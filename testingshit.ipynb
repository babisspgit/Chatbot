{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AU0012X6\\OneDrive - WSA\\Desktop\\Chatbot\\bot1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#from langchain_huggingface.llms import HuggingFacePipeline\n",
    "#from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Available Hearing Aids in the Market\\n\\n## Introduction\\nThe hearing aid market offers a wide range of devices to cater to diverse hearing needs. This document provides an overview of some of the leading models, including their key features and target audiences.\\n\\n---\\n\\n## 1. Starkey Livio AI 2400\\n\\n### Key Features:\\n- **AI Integration**: Includes artificial intelligence for advanced sound processing and health monitoring.\\n- **Connectivity**: Supports Bluetooth for calls, streaming, and app-based controls.\\n- **Health Features**: Tracks physical activities, offers fall detection, and monitors heart rate.\\n\\n### Best For:\\nIndividuals seeking a multifunctional device with both hearing and health capabilities.\\n\\n---\\n\\n## 2. Phonak AudÃ©o Paradise P90\\n\\n### Key Features:\\n- **Sound Clarity**: Provides exceptional clarity even in noisy environments.\\n- **Customizable Settings**: Multiple program options for different environments.\\n- **Rechargeable Batteries**: Long-lasting rechargeable batteries for convenience.\\n\\n### Best For:\\nActive individuals who require flexibility and adaptability in various listening conditions.\\n\\n---\\n\\n## 3. Widex Moment 440\\n\\n### Key Features:\\n- **PureSound Technology**: Delivers the most natural sound possible.\\n- **Tinnitus Relief**: Includes features designed to alleviate tinnitus symptoms.\\n- **App Integration**: Fine-tuning and personalization via the Widex app.\\n\\n### Best For:\\nPatients who prioritize sound quality and tinnitus management.\\n\\n---\\n\\n## 4. Oticon More 1\\n\\n### Key Features:\\n- **BrainHearing Technology**: Supports the brainâ€™s natural ability to process sounds.\\n- **360-Degree Sound Capture**: Provides access to all sounds in the environment.\\n- **Energy-Efficient Design**: Extended battery life for all-day use.\\n\\n### Best For:\\nIndividuals with moderate to severe hearing loss looking for enhanced sound awareness.\\n\\n---\\n\\n## Summary\\nThese devices represent the latest advancements in hearing aid technology. Choosing the right model depends on individual hearing needs, lifestyle, and preferences. Consultation with an audiologist is recommended for personalized recommendations.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#read md file\n",
    "def read_md_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "read_md_file('data/Available Hearing Aids in the Market.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AU0012X6\\OneDrive - WSA\\Desktop\\Chatbot\\bot1\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\AU0012X6\\.cache\\huggingface\\hub\\models--tiiuae--falcon-mamba-tiny-dev. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"tiiuae/falcon-mamba-tiny-dev\"\n",
    "# \"tiiuae/falcon-7b-instruct\"  # Replace with your desired Hugging Face model\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\", low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:07:56 - Loaded .env file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from dotenv import load_dotenv\n",
    "import chainlit as cl\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\au0012x6\\onedrive - wsa\\desktop\\chatbot\\bot1\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\au0012x6\\onedrive - wsa\\desktop\\chatbot\\bot1\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\au0012x6\\onedrive - wsa\\desktop\\chatbot\\bot1\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\au0012x6\\onedrive - wsa\\desktop\\chatbot\\bot1\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\au0012x6\\onedrive - wsa\\desktop\\chatbot\\bot1\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.24.0->transformers)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\au0012x6\\onedrive - wsa\\desktop\\chatbot\\bot1\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\au0012x6\\onedrive - wsa\\desktop\\chatbot\\bot1\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\au0012x6\\onedrive - wsa\\desktop\\chatbot\\bot1\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\au0012x6\\onedrive - wsa\\desktop\\chatbot\\bot1\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\au0012x6\\onedrive - wsa\\desktop\\chatbot\\bot1\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\au0012x6\\onedrive - wsa\\desktop\\chatbot\\bot1\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Using cached transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "   ---------------------------------------- 0.0/450.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/450.5 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/450.5 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 30.7/450.5 kB 330.3 kB/s eta 0:00:02\n",
      "   ----- --------------------------------- 61.4/450.5 kB 409.6 kB/s eta 0:00:01\n",
      "   ---------- --------------------------- 122.9/450.5 kB 602.4 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 184.3/450.5 kB 743.9 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 307.2/450.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  440.3/450.5 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 450.5/450.5 kB 1.3 MB/s eta 0:00:00\n",
      "Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Using cached safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.27.0 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.21.0 transformers-4.47.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-13 16:16:24 - Loaded .env file\n"
     ]
    }
   ],
   "source": [
    "import chainlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.2\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall chainlit pydantic -y\n",
    "#!pip install chainlit pydantic\n",
    "#pip install pydantic==2.10.1 chainlit\n",
    "\n",
    "# check chainlit version\n",
    "print(chainlit.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.57.4-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.8.2-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Using cached pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\au0012x6\\onedrive - wsa\\desktop\\chatbot\\bot1\\lib\\site-packages (from openai) (4.12.2)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached pydantic_core-2.27.1-cp311-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\au0012x6\\onedrive - wsa\\desktop\\chatbot\\bot1\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.57.4-py3-none-any.whl (390 kB)\n",
      "   ---------------------------------------- 0.0/390.3 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 307.2/390.3 kB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 390.3/390.3 kB 4.9 MB/s eta 0:00:00\n",
      "Using cached anyio-4.7.0-py3-none-any.whl (93 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached jiter-0.8.2-cp311-cp311-win_amd64.whl (206 kB)\n",
      "Using cached pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "Using cached pydantic_core-2.27.1-cp311-none-win_amd64.whl (2.0 MB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "   ---------------------------------------- 0.0/164.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 164.9/164.9 kB 9.7 MB/s eta 0:00:00\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: tqdm, sniffio, pydantic-core, jiter, idna, h11, distro, certifi, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.7.0 certifi-2024.12.14 distro-1.9.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 jiter-0.8.2 openai-1.57.4 pydantic-2.10.3 pydantic-core-2.27.1 sniffio-1.3.1 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import AzureOpenAI\n",
    "#from dotenv import load_dotenv\n",
    "from langchain.llms import AzureOpenAI  #\n",
    "from langchain_openai import AzureOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain - OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI model initialized successfully!\n",
      "Response from Azure OpenAI:\n",
      "content='The capital of France is Paris.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 14, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-4de057e0-ac40-4e84-a84f-dec814c6f5ba-0' usage_metadata={'input_tokens': 14, 'output_tokens': 7, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "\n",
    "def initialize_model2():\n",
    "    \"\"\"\n",
    "    Authenticate and initialize Azure OpenAI's hosted model for LangChain \n",
    "    using credentials from a .env file.\n",
    "    \"\"\"\n",
    "    # Load environment variables from .env\n",
    "    load_dotenv()\n",
    "\n",
    "    # Retrieve credentials from the environment\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "    # Validate that the required environment variables are loaded\n",
    "    if not azure_endpoint or not api_key or not api_version:\n",
    "        raise ValueError(\"Missing Azure OpenAI credentials in .env file.\")\n",
    "\n",
    "    # Initialize LangChain's Azure OpenAI wrapper\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_endpoint=azure_endpoint,  # Azure endpoint\n",
    "        api_key=api_key,                # API key for authentication\n",
    "        api_version=api_version,        # API version\n",
    "        azure_deployment=\"gpt-4o-mini\", # Replace with correct deployment name\n",
    "        temperature=0.7,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "\n",
    "# Test the Azure OpenAI model\n",
    "def test_model(llm):\n",
    "    \"\"\"\n",
    "    Send a test query to Azure OpenAI via LangChain and print the response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define a test query\n",
    "        test_query = \"What is the capital of France?\"\n",
    "\n",
    "        # Send the query to the model\n",
    "        response = llm.invoke(test_query)\n",
    "\n",
    "        # Print the response\n",
    "        print(\"Response from Azure OpenAI:\")\n",
    "        print(response)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing Azure OpenAI: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Test the initialization\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        llm = initialize_model2()\n",
    "        print(\"Azure OpenAI model initialized successfully!\")\n",
    "\n",
    "        # Test the model with a simple query\n",
    "        test_model(llm)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Azure OpenAI model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI - Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI initialized successfully!\n",
      "2024-12-17 16:06:11 - HTTP Request: POST https://main-azure-openai-swe.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n",
      "Response from Azure OpenAI:\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "def initialize_model():\n",
    "    \"\"\"\n",
    "    Authenticate and initialize Azure OpenAI's hosted model for LangChain \n",
    "    using credentials from a .env file.\n",
    "    \"\"\"\n",
    "    # Load environment variables from .env\n",
    "    load_dotenv()\n",
    "\n",
    "    # Retrieve credentials from the environment\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "    # Validate that the required environment variables are loaded\n",
    "    if not azure_endpoint or not api_key or not api_version:\n",
    "        raise ValueError(\"Missing Azure OpenAI credentials in .env file.\")\n",
    "\n",
    "    # Initialize LangChain's Azure OpenAI wrapper\n",
    "    llm = AzureOpenAI(\n",
    "        azure_endpoint=azure_endpoint,  # Azure endpoint\n",
    "        api_key=api_key,                # API key for authentication\n",
    "        api_version=api_version        # API version\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "\n",
    "# Test the Azure OpenAI model\n",
    "def test_model(client):\n",
    "    \"\"\"\n",
    "    Test the Azure OpenAI model by sending a prompt and printing the response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define a test prompt\n",
    "        test_prompt = \"What is the capital of France?\"\n",
    "        \n",
    "        # Call the Azure OpenAI model (assuming 'gpt-35-turbo' is the deployment name)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # Replace with the correct deployment name\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": test_prompt}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        # Print the response\n",
    "        print(\"Response from Azure OpenAI:\")\n",
    "        print(response.choices[0].message.content)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing Azure OpenAI: {e}\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the model\n",
    "    client = initialize_model()\n",
    "    print(\"Azure OpenAI initialized successfully!\")\n",
    "\n",
    "    # Test the model with a simple query\n",
    "    test_model(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for AzureOpenAI\n  Value error, Must provide either the `api_version` argument or the `OPENAI_API_VERSION` environment variable [type=value_error, input_value={'deployment_name': 'gpt-4o', 'model_kwargs': {}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI\n\u001b[1;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mAzureOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeployment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AU0012X6\\OneDrive - WSA\\Desktop\\Chatbot\\bot1\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AU0012X6\\OneDrive - WSA\\Desktop\\Chatbot\\bot1\\Lib\\site-packages\\pydantic\\main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    221\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for AzureOpenAI\n  Value error, Must provide either the `api_version` argument or the `OPENAI_API_VERSION` environment variable [type=value_error, input_value={'deployment_name': 'gpt-4o', 'model_kwargs': {}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    deployment_name=\"gpt-4o\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
